{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# dados fin\n",
    "import yfinance as yf\n",
    "\n",
    "start = '2021-01-01'\n",
    "end = '2023-01-01'\n",
    "\n",
    "data = yf.download('AAPL', \n",
    "                   start=start,\n",
    "                   end=end)\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.to_csv('data_/AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>133.610001</td>\n",
       "      <td>126.760002</td>\n",
       "      <td>129.410004</td>\n",
       "      <td>127.503639</td>\n",
       "      <td>143301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>128.889999</td>\n",
       "      <td>131.740005</td>\n",
       "      <td>128.429993</td>\n",
       "      <td>131.009995</td>\n",
       "      <td>129.080048</td>\n",
       "      <td>97664900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>127.720001</td>\n",
       "      <td>131.050003</td>\n",
       "      <td>126.379997</td>\n",
       "      <td>126.599998</td>\n",
       "      <td>124.735023</td>\n",
       "      <td>155088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>128.360001</td>\n",
       "      <td>131.630005</td>\n",
       "      <td>127.860001</td>\n",
       "      <td>130.919998</td>\n",
       "      <td>128.991379</td>\n",
       "      <td>109578200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>132.429993</td>\n",
       "      <td>132.630005</td>\n",
       "      <td>130.229996</td>\n",
       "      <td>132.050003</td>\n",
       "      <td>130.104752</td>\n",
       "      <td>105158200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2021-01-04  133.520004  133.610001  126.760002  129.410004  127.503639   \n",
       "1  2021-01-05  128.889999  131.740005  128.429993  131.009995  129.080048   \n",
       "2  2021-01-06  127.720001  131.050003  126.379997  126.599998  124.735023   \n",
       "3  2021-01-07  128.360001  131.630005  127.860001  130.919998  128.991379   \n",
       "4  2021-01-08  132.429993  132.630005  130.229996  132.050003  130.104752   \n",
       "\n",
       "      Volume  \n",
       "0  143301900  \n",
       "1   97664900  \n",
       "2  155088000  \n",
       "3  109578200  \n",
       "4  105158200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregando os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_average(data, window):\n",
    "    return data['Close'].rolling(window=window).mean()\n",
    "\n",
    "# Vamos testar para diferentes períodos de média móvel (por exemplo, de 5 a 50 dias)\n",
    "moving_averages = []\n",
    "for window in range(5, 51):\n",
    "    moving_averages.append(calculate_moving_average(data, window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_from_moving_average(data, moving_average):\n",
    "    return abs(data['Close'] - moving_average)\n",
    "\n",
    "distance_from_moving_averages = []\n",
    "for moving_average in moving_averages:\n",
    "    distance_from_moving_averages.append(calculate_distance_from_moving_average(data, moving_average))\n",
    "\n",
    "# Vamos calcular a média das distâncias para cada período\n",
    "mean_distances = [dist.mean() for dist in distance_from_moving_averages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A melhor média móvel é de 5 dias.\n"
     ]
    }
   ],
   "source": [
    "best_window = mean_distances.index(min(mean_distances)) + 5  # +5 pois começamos a partir da janela 5\n",
    "\n",
    "print(f'A melhor média móvel é de {best_window} dias.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'window' for estimator LinearRegression(). Valid parameters are: ['copy_X', 'fit_intercept', 'n_jobs', 'normalize', 'positive'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py\", line 262, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 117, in __call__\n    return self.function(*args, **kwargs)\n  File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 674, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py\", line 246, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'window' for estimator LinearRegression(). Valid parameters are: ['copy_X', 'fit_intercept', 'n_jobs', 'normalize', 'positive'].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT-repository\\github-machine-learning-trading\\machine-learning-trading\\start.ipynb Cell 6\u001b[0m in \u001b[0;36m<cell line: 36>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#W6sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(estimator\u001b[39m=\u001b[39mmodel, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39mtscv, scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mneg_mean_squared_error\u001b[39m\u001b[39m'\u001b[39m, n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#W6sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39m# Executando a busca pelo melhor período de média móvel\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#W6sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#W6sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# Extraindo os resultados\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#W6sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m best_window \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mbest_params_[\u001b[39m'\u001b[39m\u001b[39mwindow\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1375\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1373\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1374\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1375\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1053\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1056\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1057\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1058\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    934\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 935\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    936\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    543\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:446\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    445\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 446\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    447\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\concurrent\\futures\\_base.py:391\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    389\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    390\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    392\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    393\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    394\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'window' for estimator LinearRegression(). Valid parameters are: ['copy_X', 'fit_intercept', 'n_jobs', 'normalize', 'positive']."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Carregando os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Definindo a função para calcular a distância da média móvel\n",
    "def calculate_distance_from_moving_average(data, window):\n",
    "    moving_average = data['Close'].rolling(window=window).mean()\n",
    "    return abs(data['Close'] - moving_average)\n",
    "\n",
    "# Criação de um array de períodos de média móvel a serem testados\n",
    "window_sizes = np.arange(5, 51)\n",
    "\n",
    "# Preparação dos dados para validação cruzada\n",
    "X = data[['Close']]\n",
    "y = data['Close']\n",
    "\n",
    "# Definindo o modelo de regressão linear\n",
    "model = LinearRegression()\n",
    "\n",
    "# Definindo os parâmetros a serem testados no GridSearch\n",
    "param_grid = {'window': window_sizes}\n",
    "\n",
    "# Definindo a estratégia de validação cruzada\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Criando o objeto GridSearchCV\n",
    "grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Executando a busca pelo melhor período de média móvel\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Extraindo os resultados\n",
    "best_window = grid_search.best_params_['window']\n",
    "best_mse = -grid_search.best_score_\n",
    "\n",
    "print(f\"A melhor média móvel é de {best_window} dias, com MSE de {best_mse}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Close  Buy_Signal  Sell_Signal       PnL  Cumulative_Return\n",
      "Date                                                                        \n",
      "2022-12-16  134.509995           0           -1 -0.000000           9.162907\n",
      "2022-12-19  132.369995           0           -1 -0.000000           9.162907\n",
      "2022-12-20  132.300003           0           -1 -0.000529           9.158062\n",
      "2022-12-21  135.449997           1            0  0.000000           9.158062\n",
      "2022-12-22  132.229996           0           -1 -0.000000           9.158062\n",
      "2022-12-23  131.860001           0           -1 -0.000000           9.158062\n",
      "2022-12-27  130.029999           0           -1 -0.000000           9.158062\n",
      "2022-12-28  126.040001           0           -1 -0.000000           9.158062\n",
      "2022-12-29  129.610001           0           -1  0.028324           9.417458\n",
      "2022-12-30  129.929993           1            0       NaN                NaN\n"
     ]
    }
   ],
   "source": [
    "# Calculando a melhor média móvel com o período encontrado pelo GridSearch\n",
    "best_moving_average = data['Close'].rolling(window=best_window).mean()\n",
    "\n",
    "# Criando colunas para sinal de compra (1), sinal de venda (-1) e sinal neutro (0)\n",
    "data['Buy_Signal'] = np.where(data['Close'] > best_moving_average, 1, 0)\n",
    "data['Sell_Signal'] = np.where(data['Close'] < best_moving_average, -1, 0)\n",
    "\n",
    "# Calculando o lucro e a perda (PnL) da estratégia\n",
    "data['PnL'] = data['Close'].pct_change() * data['Buy_Signal'].shift(-1)\n",
    "\n",
    "# Calculando o retorno cumulativo\n",
    "data['Cumulative_Return'] = (1 + data['PnL']).cumprod()\n",
    "\n",
    "# Visualizando os resultados\n",
    "print(data[['Close', 'Buy_Signal', 'Sell_Signal', 'PnL', 'Cumulative_Return']].tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno anualizado: nan%\n",
      "Índice de Sharpe: 0.35\n",
      "Drawdown máximo: -4.17%\n"
     ]
    }
   ],
   "source": [
    "# Cálculo do retorno anualizado\n",
    "days_per_year = 252  # Supondo que há 252 dias úteis em um ano\n",
    "total_trading_days = data['PnL'].count()\n",
    "annualized_return = ((data['Cumulative_Return'].iloc[-1]) ** (days_per_year / total_trading_days)) - 1\n",
    "\n",
    "# Cálculo do risco livre (pode ser ajustado de acordo com a taxa livre de risco atual)\n",
    "risk_free_rate = 0.03  # Exemplo: 3% ao ano\n",
    "\n",
    "# Cálculo do índice de Sharpe\n",
    "daily_returns = data['PnL']\n",
    "daily_risk_free_rate = (1 + risk_free_rate) ** (1 / days_per_year) - 1\n",
    "daily_volatility = daily_returns.std()\n",
    "sharpe_ratio = (daily_returns.mean() - daily_risk_free_rate) / daily_volatility\n",
    "\n",
    "# Cálculo do drawdown máximo\n",
    "cumulative_returns = data['Cumulative_Return']\n",
    "previous_peaks = cumulative_returns.cummax()\n",
    "drawdowns = cumulative_returns / previous_peaks - 1\n",
    "max_drawdown = drawdowns.min()\n",
    "\n",
    "# Visualizando as métricas\n",
    "print(f\"Retorno anualizado: {annualized_return:.2%}\")\n",
    "print(f\"Índice de Sharpe: {sharpe_ratio:.2f}\")\n",
    "print(f\"Drawdown máximo: {max_drawdown:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.44\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        57\n",
      "         1.0       0.44      1.00      0.61        44\n",
      "\n",
      "    accuracy                           0.44       101\n",
      "   macro avg       0.22      0.50      0.30       101\n",
      "weighted avg       0.19      0.44      0.26       101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Preparação dos dados para o modelo de machine learning\n",
    "X = data[['Close']].shift(1).dropna()  # Usamos o preço de fechamento do dia anterior como feature\n",
    "y = data['Buy_Signal'].shift(-1).dropna()  # O sinal de compra do próximo dia é o rótulo\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Criação e treinamento do modelo de Regressão Logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_output = classification_report(y_test, y_pred)\n",
    "\n",
    "# Visualizando os resultados\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(\"Relatório de Classificação:\\n\", classification_report_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo SVM: 0.44\n",
      "Relatório de Classificação do modelo SVM:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        57\n",
      "         1.0       0.44      1.00      0.61        44\n",
      "\n",
      "    accuracy                           0.44       101\n",
      "   macro avg       0.22      0.50      0.30       101\n",
      "weighted avg       0.19      0.44      0.26       101\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Criação e treinamento do modelo SVM\n",
    "model_svm = SVC(kernel='linear', C=1.0)\n",
    "model_svm.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo SVM\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "classification_report_output_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "# Visualizando os resultados do modelo SVM\n",
    "print(f\"Acurácia do modelo SVM: {accuracy_svm:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo SVM:\\n\", classification_report_output_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Obtaining dependency information for tensorflow from https://files.pythonhosted.org/packages/ba/7c/b971f2485155917ecdcebb210e021e36a6b65457394590be01cc61515310/tensorflow-2.13.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-win_amd64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-intel==2.13.0 (from tensorflow)\n",
      "  Obtaining dependency information for tensorflow-intel==2.13.0 from https://files.pythonhosted.org/packages/40/fa/98115f6fe4d92e1962f549917be2dc8e369853b7e404191996fedaaf4dd6/tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "     ------------------------------------ 126.5/126.5 kB 135.4 kB/s eta 0:00:00\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for flatbuffers>=23.1.21 from https://files.pythonhosted.org/packages/6f/12/d5c79ee252793ffe845d58a913197bfa02ae9a0b5c9bc3dc4b58d477b9e7/flatbuffers-23.5.26-py2.py3-none-any.whl.metadata\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "     -------------------------------------- 57.5/57.5 kB 177.6 kB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for h5py>=2.9.0 from https://files.pythonhosted.org/packages/e2/c4/6f8dae1530d57a6122fd5b72c750187484acbe612f630cb2179e4bcb12c1/h5py-3.9.0-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading h5py-3.9.0-cp310-cp310-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for libclang>=13.0.0 from https://files.pythonhosted.org/packages/02/8c/dc970bc00867fe290e8c8a7befa1635af716a9ebdfe3fb9dce0ca4b522ce/libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata\n",
      "  Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.23.0)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "     --------------------------------------- 65.5/65.5 kB 69.3 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (23.1)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 from https://files.pythonhosted.org/packages/80/70/dc63d340d27b8ff22022d7dd14b8d6d68b479a003eacdc4507150a286d9a/protobuf-4.23.4-cp310-abi3-win_amd64.whl.metadata\n",
      "  Downloading protobuf-4.23.4-cp310-abi3-win_amd64.whl.metadata (540 bytes)\n",
      "Requirement already satisfied: setuptools in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (68.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.13.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-win_amd64.whl (36 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for grpcio<2.0,>=1.24.3 from https://files.pythonhosted.org/packages/3f/fe/b332c682f682745230083023935a06f21e029cb8ef01900531a89b47217c/grpcio-1.56.2-cp310-cp310-win_amd64.whl.metadata\n",
      "  Downloading grpcio-1.56.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "     ---------------------------------------- 5.6/5.6 MB 201.4 kB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for tensorflow-estimator<2.14,>=2.13.0 from https://files.pythonhosted.org/packages/72/5c/c318268d96791c6222ad7df1651bbd1b2409139afeb6f468c0f327177016/tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for keras<2.14,>=2.13.1 from https://files.pythonhosted.org/packages/2e/f3/19da7511b45e80216cbbd9467137b2d28919c58ba1ccb971435cb631e470/keras-2.13.1-py3-none-any.whl.metadata\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 3.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.13.0->tensorflow) (0.41.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for google-auth<3,>=1.6.3 from https://files.pythonhosted.org/packages/9c/8d/bff87fc722553a5691d8514da5523c23547f3894189ba03b57592e37bdc2/google_auth-2.22.0-py2.py3-none-any.whl.metadata\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
      "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for tensorboard-data-server<0.8.0,>=0.7.0 from https://files.pythonhosted.org/packages/da/61/6e9ff8258422d287eec718872fb71e05324356722ab658c8afda25f51539/tensorboard_data_server-0.7.1-py3-none-any.whl.metadata\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.2.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Obtaining dependency information for cachetools<6.0,>=2.0.0 from https://files.pythonhosted.org/packages/a9/c9/c8a7710f2cedcb1db9224fdd4d8307c9e48cbddc46c18b515fefc0f1abbe/cachetools-5.3.1-py3-none-any.whl.metadata\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "     -------------------------------------- 181.3/181.3 kB 3.6 MB/s eta 0:00:00\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (1.26.10)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2022.6.15)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow)\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "     ---------------------------------------- 83.9/83.9 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\caíque miranda\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-intel==2.13.0->tensorflow) (3.2.2)\n",
      "Downloading tensorflow-2.13.0-cp310-cp310-win_amd64.whl (1.9 kB)\n",
      "Downloading tensorflow_intel-2.13.0-cp310-cp310-win_amd64.whl (276.5 MB)\n",
      "   ---------------------------------------- 276.5/276.5 MB 4.1 MB/s eta 0:00:00\n",
      "Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Downloading grpcio-1.56.2-cp310-cp310-win_amd64.whl (4.2 MB)\n",
      "   ---------------------------------------- 4.2/4.2 MB 5.8 MB/s eta 0:00:00\n",
      "Downloading h5py-3.9.0-cp310-cp310-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 2.7/2.7 MB 6.1 MB/s eta 0:00:00\n",
      "Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 1.7/1.7 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading libclang-16.0.6-py2.py3-none-win_amd64.whl (24.4 MB)\n",
      "   ---------------------------------------- 24.4/24.4 MB 6.3 MB/s eta 0:00:00\n",
      "Downloading protobuf-4.23.4-cp310-abi3-win_amd64.whl (422 kB)\n",
      "   ---------------------------------------- 422.5/422.5 kB 6.6 MB/s eta 0:00:00\n",
      "Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "   ---------------------------------------- 440.8/440.8 kB 9.2 MB/s eta 0:00:00\n",
      "Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "   ---------------------------------------- 181.8/181.8 kB 5.5 MB/s eta 0:00:00\n",
      "Downloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
      "   ---------------------------------------- 94.2/94.2 kB 5.6 MB/s eta 0:00:00\n",
      "Downloading tensorboard_data_server-0.7.1-py3-none-any.whl (2.4 kB)\n",
      "Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, protobuf, opt-einsum, markdown, keras, h5py, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.7.0\n",
      "    Uninstalling typing_extensions-4.7.0:\n",
      "      Successfully uninstalled typing_extensions-4.7.0\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.2 h5py-3.9.0 keras-2.13.1 libclang-16.0.6 markdown-3.4.4 opt-einsum-3.3.0 protobuf-4.23.4 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-intel-2.13.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.3.0 typing-extensions-4.5.0 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Criação e treinamento do modelo de Rede Neural\n",
    "model_nn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(1,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_nn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_nn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_nn = model_nn.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo de Rede Neural\n",
    "accuracy_nn = accuracy_score(y_test, y_pred_nn)\n",
    "classification_report_output_nn = classification_report(y_test, y_pred_nn)\n",
    "\n",
    "# Visualizando os resultados do modelo de Rede Neural\n",
    "print(f\"Acurácia do modelo de Rede Neural: {accuracy_nn:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo de Rede Neural:\\n\", classification_report_output_nn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:834: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\range.py:385\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    384\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 385\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_range\u001b[39m.\u001b[39;49mindex(new_key)\n\u001b[0;32m    386\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: 0 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT-repository\\github-machine-learning-trading\\machine-learning-trading\\start.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#X15sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_arima_fit \u001b[39m=\u001b[39m model_arima\u001b[39m.\u001b[39mfit()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#X15sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Realizando as previsões para o conjunto de teste\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#X15sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m y_pred_arima \u001b[39m=\u001b[39m model_arima_fit\u001b[39m.\u001b[39;49mforecast(steps\u001b[39m=\u001b[39;49m\u001b[39mlen\u001b[39;49m(X_test))[\u001b[39m0\u001b[39;49m]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39m# Convertendo as previsões para sinais de compra e venda\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m y_pred_arima_signals \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mwhere(y_pred_arima \u001b[39m>\u001b[39m X_test[\u001b[39m'\u001b[39m\u001b[39mClose\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:958\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    955\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[key]\n\u001b[0;32m    957\u001b[0m \u001b[39melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 958\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_value(key)\n\u001b[0;32m    960\u001b[0m \u001b[39mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    961\u001b[0m     \u001b[39m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    962\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    963\u001b[0m         \u001b[39m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1069\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1066\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_values[label]\n\u001b[0;32m   1068\u001b[0m \u001b[39m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1069\u001b[0m loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindex\u001b[39m.\u001b[39;49mget_loc(label)\n\u001b[0;32m   1070\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex\u001b[39m.\u001b[39m_get_values_for_loc(\u001b[39mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\range.py:387\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    385\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_range\u001b[39m.\u001b[39mindex(new_key)\n\u001b[0;32m    386\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[1;32m--> 387\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    389\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Criação e treinamento do modelo ARIMA\n",
    "model_arima = ARIMA(X_train, order=(5, 1, 0))  # Ordem (p, d, q) ajustável conforme necessário\n",
    "model_arima_fit = model_arima.fit()\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_arima = model_arima_fit.forecast(steps=len(X_test))[0]\n",
    "\n",
    "# Convertendo as previsões para sinais de compra e venda\n",
    "y_pred_arima_signals = np.where(y_pred_arima > X_test['Close'], 1, -1)\n",
    "\n",
    "# Avaliando o desempenho do modelo ARIMA\n",
    "accuracy_arima = accuracy_score(y_test, y_pred_arima_signals)\n",
    "classification_report_output_arima = classification_report(y_test, y_pred_arima_signals)\n",
    "\n",
    "# Visualizando os resultados do modelo ARIMA\n",
    "print(f\"Acurácia do modelo ARIMA: {accuracy_arima:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo ARIMA:\\n\", classification_report_output_arima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT-repository\\github-machine-learning-trading\\machine-learning-trading\\start.ipynb Cell 14\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m y_pred_lstm \u001b[39m=\u001b[39m model_lstm\u001b[39m.\u001b[39mpredict(X_test_lstm)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Avaliando o desempenho do modelo LSTM\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m accuracy_lstm \u001b[39m=\u001b[39m accuracy_score(y_test, y_pred_lstm)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#X20sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m classification_report_output_lstm \u001b[39m=\u001b[39m classification_report(y_test, y_pred_lstm)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/start.ipynb#X20sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m# Visualizando os resultados do modelo LSTM\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:211\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[39m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \n\u001b[0;32m    147\u001b[0m \u001b[39mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[39m0.5\u001b[39;00m\n\u001b[0;32m    208\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[39m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 211\u001b[0m y_type, y_true, y_pred \u001b[39m=\u001b[39m _check_targets(y_true, y_pred)\n\u001b[0;32m    212\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    213\u001b[0m \u001b[39mif\u001b[39;00m y_type\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39mmultilabel\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:93\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     90\u001b[0m     y_type \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mmulticlass\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[0;32m     92\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(y_type) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m---> 93\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mClassification metrics can\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt handle a mix of \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m targets\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m     95\u001b[0m             type_true, type_pred\n\u001b[0;32m     96\u001b[0m         )\n\u001b[0;32m     97\u001b[0m     )\n\u001b[0;32m     99\u001b[0m \u001b[39m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m y_type \u001b[39m=\u001b[39m y_type\u001b[39m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Preparação dos dados para o modelo LSTM\n",
    "X_train_lstm = X_train.values.reshape(-1, 1, 1)\n",
    "X_test_lstm = X_test.values.reshape(-1, 1, 1)\n",
    "\n",
    "# Criação e treinamento do modelo LSTM\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, input_shape=(1, 1)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_lstm.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_lstm = model_lstm.predict(X_test_lstm)\n",
    "\n",
    "# Avaliando o desempenho do modelo LSTM\n",
    "accuracy_lstm = accuracy_score(y_test, y_pred_lstm)\n",
    "classification_report_output_lstm = classification_report(y_test, y_pred_lstm)\n",
    "\n",
    "# Visualizando os resultados do modelo LSTM\n",
    "print(f\"Acurácia do modelo LSTM: {accuracy_lstm:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo LSTM:\\n\", classification_report_output_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.action_space = gym.spaces.Discrete(3)  # 0: não fazer nada, 1: comprar, 2: vender\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.profit = 0\n",
    "        self.holdings = 0\n",
    "        self.initial_balance = 1000  # Defina o saldo inicial desejado\n",
    "        self.balance = self.initial_balance\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return np.array([self.data['Close'].iloc[self.current_step] / self.data['Close'].max()])\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if action == 1:  # Comprar\n",
    "            if self.holdings == 0:\n",
    "                self.holdings = self.balance / self.data['Close'].iloc[self.current_step]\n",
    "                self.balance = 0\n",
    "\n",
    "        elif action == 2:  # Vender\n",
    "            if self.holdings > 0:\n",
    "                self.balance = self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "                self.holdings = 0\n",
    "\n",
    "        # Calcular a recompensa\n",
    "        current_balance = self.balance + self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "        self.profit = (current_balance - self.initial_balance) / self.initial_balance\n",
    "\n",
    "        # Verificar se o episódio terminou (pode adicionar outras condições de término)\n",
    "        done = self.current_step == len(self.data) - 1\n",
    "\n",
    "        # Definir a recompensa\n",
    "        if done:\n",
    "            reward = self.profit\n",
    "        else:\n",
    "            reward = self.profit - 0.001  # Penalidade diária para incentivar ações mais curtas\n",
    "\n",
    "        return self._next_observation(), reward, done, {}\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('caminho/do/arquivo.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Criação do ambiente de trading\n",
    "env = TradingEnv(data)\n",
    "\n",
    "# Implementação do algoritmo Q-Learning (neste exemplo, não estamos realizando o treinamento, mas você pode implementá-lo)\n",
    "def q_learning(env, num_episodes=100):\n",
    "    q_table = np.zeros((len(env.observation_space.high), env.action_space.n))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Atualizar a tabela Q\n",
    "            q_table[state, action] = (1 - 0.1) * q_table[state, action] + 0.1 * (reward + 0.9 * np.max(q_table[new_state]))\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "q_learning(env)\n",
    "\n",
    "# Teste da estratégia treinada\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "final_profit = env.profit\n",
    "print(f\"Lucro final da estratégia: {final_profit:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('caminho/do/arquivo.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data[['Close']])\n",
    "X, y = [], []\n",
    "for i in range(len(data_scaled) - 1):\n",
    "    X.append(data_scaled[i])\n",
    "    y.append(data_scaled[i + 1][0])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Reshape dos dados para o formato [amostras, tempo, características]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Criação e treinamento do modelo LSTM\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, input_shape=(1, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model_lstm.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_lstm = model_lstm.predict(X_test)\n",
    "\n",
    "# Transformando as previsões para a escala original\n",
    "y_pred_lstm = scaler.inverse_transform(y_pred_lstm)\n",
    "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Convertendo as previsões para sinais de compra e venda\n",
    "y_pred_signals = np.where(y_pred_lstm > y_test, 1, -1)\n",
    "\n",
    "# Avaliando o desempenho do modelo LSTM\n",
    "accuracy_lstm = accuracy_score(y_test, y_pred_signals)\n",
    "classification_report_output_lstm = classification_report(y_test, y_pred_signals)\n",
    "\n",
    "# Visualizando os resultados do modelo LSTM\n",
    "print(f\"Acurácia do modelo LSTM: {accuracy_lstm:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo LSTM:\\n\", classification_report_output_lstm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('caminho/do/arquivo.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data[['Close']])\n",
    "X, y = [], []\n",
    "look_back = 10  # Defina o tamanho da janela temporal\n",
    "for i in range(len(data_scaled) - look_back):\n",
    "    X.append(data_scaled[i:i + look_back])\n",
    "    y.append(data_scaled[i + look_back])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo CNN\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(look_back, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model_cnn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_cnn = model_cnn.predict(X_test)\n",
    "\n",
    "# Transformando as previsões para a escala original\n",
    "y_pred_cnn = scaler.inverse_transform(y_pred_cnn)\n",
    "y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Convertendo as previsões para sinais de compra e venda\n",
    "y_pred_signals = np.where(y_pred_cnn > y_test, 1, -1)\n",
    "\n",
    "# Avaliando o desempenho do modelo CNN\n",
    "accuracy_cnn = accuracy_score(y_test, y_pred_signals)\n",
    "classification_report_output_cnn = classification_report(y_test, y_pred_signals)\n",
    "\n",
    "# Visualizando os resultados do modelo CNN\n",
    "print(f\"Acurácia do modelo CNN: {accuracy_cnn:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo CNN:\\n\", classification_report_output_cnn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('caminho/do/arquivo.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo XGBoost\n",
    "model_xgb = XGBClassifier()\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo XGBoost\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "classification_report_output_xgb = classification_report(y_test, y_pred_xgb)\n",
    "\n",
    "# Visualizando os resultados do modelo XGBoost\n",
    "print(f\"Acurácia do modelo XGBoost: {accuracy_xgb:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo XGBoost:\\n\", classification_report_output_xgb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('caminho/do/arquivo.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo Random Forest\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo Random Forest\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "classification_report_output_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "# Visualizando os resultados do modelo Random Forest\n",
    "print(f\"Acurácia do modelo Random Forest: {accuracy_rf:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo Random Forest:\\n\", classification_report_output_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('caminho/do/arquivo.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Normalização dos dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Reshape dos dados para o formato adequado\n",
    "look_back = 10\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], look_back, 1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], look_back, 1))\n",
    "\n",
    "# Criação e treinamento do modelo híbrido LSTM + CNN\n",
    "model_hybrid = Sequential([\n",
    "    LSTM(64, input_shape=(look_back, 1), return_sequences=True),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_hybrid.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_hybrid = model_hybrid.predict_classes(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo híbrido LSTM + CNN\n",
    "accuracy_hybrid = accuracy_score(y_test, y_pred_hybrid)\n",
    "classification_report_output_hybrid = classification_report(y_test, y_pred_hybrid)\n",
    "\n",
    "# Visualizando os resultados do modelo híbrido LSTM + CNN\n",
    "print(f\"Acurácia do modelo híbrido LSTM + CNN: {accuracy_hybrid:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo híbrido LSTM + CNN:\\n\", classification_report_output_hybrid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.action_space = spaces.Discrete(3)  # 0: não fazer nada, 1: comprar, 2: vender\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.profit = 0\n",
    "        self.holdings = 0\n",
    "        self.initial_balance = 1000  # Defina o saldo inicial desejado\n",
    "        self.balance = self.initial_balance\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return np.array([self.data['Close'].iloc[self.current_step] / self.data['Close'].max()])\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if action == 1:  # Comprar\n",
    "            if self.holdings == 0:\n",
    "                self.holdings = self.balance / self.data['Close'].iloc[self.current_step]\n",
    "                self.balance = 0\n",
    "\n",
    "        elif action == 2:  # Vender\n",
    "            if self.holdings > 0:\n",
    "                self.balance = self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "                self.holdings = 0\n",
    "\n",
    "        # Calcular a recompensa\n",
    "        current_balance = self.balance + self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "        self.profit = (current_balance - self.initial_balance) / self.initial_balance\n",
    "\n",
    "        # Verificar se o episódio terminou (pode adicionar outras condições de término)\n",
    "        done = self.current_step == len(self.data) - 1\n",
    "\n",
    "        # Definir a recompensa\n",
    "        if done:\n",
    "            reward = self.profit\n",
    "        else:\n",
    "            reward = self.profit - 0.001  # Penalidade diária para incentivar ações mais curtas\n",
    "\n",
    "        return self._next_observation(), reward, done, {}\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('caminho/do/arquivo.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Criação do ambiente de trading\n",
    "env = TradingEnv(data)\n",
    "\n",
    "# Implementação do algoritmo Deep Q-Network (DQN)\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Fator de desconto\n",
    "        self.epsilon = 1.0  # Probabilidade de exploração inicial\n",
    "        self.epsilon_decay = 0.995  # Taxa de decaimento da probabilidade de exploração\n",
    "        self.epsilon_min = 0.01  # Probabilidade mínima de exploração\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Configuração do agente DQN\n",
    "state_size = 1\n",
    "action_size = 3\n",
    "batch_size = 32\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Treinamento do agente DQN\n",
    "for e in range(100):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"Época: {}/{}, Lucro: {:.2f}\"\n",
    "                  .format(e, 100, env.profit))\n",
    "            break\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "# Teste da estratégia aprendida\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    next_state, _, done, _ = env.step(action)\n",
    "    state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "final_profit = env.profit\n",
    "print(f\"Lucro final da estratégia: {final_profit:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
