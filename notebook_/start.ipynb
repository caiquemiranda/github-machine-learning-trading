{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# dados fin\n",
    "import yfinance as yf\n",
    "\n",
    "start = '2021-01-01'\n",
    "end = '2023-01-01'\n",
    "\n",
    "data = yf.download('AAPL', \n",
    "                   start=start,\n",
    "                   end=end)\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.to_csv('data_/AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2021-01-04</td>\n",
       "      <td>133.520004</td>\n",
       "      <td>133.610001</td>\n",
       "      <td>126.760002</td>\n",
       "      <td>129.410004</td>\n",
       "      <td>127.503662</td>\n",
       "      <td>143301900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2021-01-05</td>\n",
       "      <td>128.889999</td>\n",
       "      <td>131.740005</td>\n",
       "      <td>128.429993</td>\n",
       "      <td>131.009995</td>\n",
       "      <td>129.080078</td>\n",
       "      <td>97664900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2021-01-06</td>\n",
       "      <td>127.720001</td>\n",
       "      <td>131.050003</td>\n",
       "      <td>126.379997</td>\n",
       "      <td>126.599998</td>\n",
       "      <td>124.735031</td>\n",
       "      <td>155088000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021-01-07</td>\n",
       "      <td>128.360001</td>\n",
       "      <td>131.630005</td>\n",
       "      <td>127.860001</td>\n",
       "      <td>130.919998</td>\n",
       "      <td>128.991425</td>\n",
       "      <td>109578200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021-01-08</td>\n",
       "      <td>132.429993</td>\n",
       "      <td>132.630005</td>\n",
       "      <td>130.229996</td>\n",
       "      <td>132.050003</td>\n",
       "      <td>130.104767</td>\n",
       "      <td>105158200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        Open        High         Low       Close   Adj Close  \\\n",
       "0  2021-01-04  133.520004  133.610001  126.760002  129.410004  127.503662   \n",
       "1  2021-01-05  128.889999  131.740005  128.429993  131.009995  129.080078   \n",
       "2  2021-01-06  127.720001  131.050003  126.379997  126.599998  124.735031   \n",
       "3  2021-01-07  128.360001  131.630005  127.860001  130.919998  128.991425   \n",
       "4  2021-01-08  132.429993  132.630005  130.229996  132.050003  130.104767   \n",
       "\n",
       "      Volume  \n",
       "0  143301900  \n",
       "1   97664900  \n",
       "2  155088000  \n",
       "3  109578200  \n",
       "4  105158200  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregando os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_average(data, window):\n",
    "    return data['Close'].rolling(window=window).mean()\n",
    "\n",
    "# Vamos testar para diferentes períodos de média móvel (por exemplo, de 5 a 50 dias)\n",
    "moving_averages = []\n",
    "for window in range(5, 51):\n",
    "    moving_averages.append(calculate_moving_average(data, window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_from_moving_average(data, moving_average):\n",
    "    return abs(data['Close'] - moving_average)\n",
    "\n",
    "distance_from_moving_averages = []\n",
    "for moving_average in moving_averages:\n",
    "    distance_from_moving_averages.append(calculate_distance_from_moving_average(data, moving_average))\n",
    "\n",
    "# Vamos calcular a média das distâncias para cada período\n",
    "mean_distances = [dist.mean() for dist in distance_from_moving_averages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A melhor média móvel é de 5 dias.\n"
     ]
    }
   ],
   "source": [
    "best_window = mean_distances.index(min(mean_distances)) + 5  # +5 pois começamos a partir da janela 5\n",
    "\n",
    "print(f'A melhor média móvel é de {best_window} dias.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A melhor média móvel é de 50 dias, com MSE de -105.27.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Carregando os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Definindo a função para calcular a distância da média móvel simples\n",
    "def calculate_distance_from_simple_moving_average(data, window):\n",
    "    moving_average = data['Close'].rolling(window=window).mean()\n",
    "    return abs(data['Close'] - moving_average)\n",
    "\n",
    "# Criação de um array de períodos de média móvel a serem testados\n",
    "window_sizes = np.arange(5, 51)\n",
    "\n",
    "# Preparação dos dados para validação cruzada\n",
    "X = data[['Close']]\n",
    "y = data['Close']\n",
    "\n",
    "# Definindo o modelo de regressão linear\n",
    "model = LinearRegression()\n",
    "\n",
    "# Inicializando as listas para armazenar os resultados\n",
    "best_window = None\n",
    "best_mse = float('inf')\n",
    "mse_values = []\n",
    "\n",
    "# Realizando a busca pelo melhor período de média móvel\n",
    "for window in window_sizes:\n",
    "    mse = -np.mean(calculate_distance_from_simple_moving_average(data, window)**2)\n",
    "    mse_values.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_window = window\n",
    "\n",
    "print(f\"A melhor média móvel é de {best_window} dias, com MSE de {best_mse:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Close  Buy_Signal  Sell_Signal  PnL  Cumulative_Return\n",
      "Date                                                                   \n",
      "2022-12-16  134.509995           0           -1 -0.0           2.397015\n",
      "2022-12-19  132.369995           0           -1 -0.0           2.397015\n",
      "2022-12-20  132.300003           0           -1 -0.0           2.397015\n",
      "2022-12-21  135.449997           0           -1  0.0           2.397015\n",
      "2022-12-22  132.229996           0           -1 -0.0           2.397015\n",
      "2022-12-23  131.860001           0           -1 -0.0           2.397015\n",
      "2022-12-27  130.029999           0           -1 -0.0           2.397015\n",
      "2022-12-28  126.040001           0           -1 -0.0           2.397015\n",
      "2022-12-29  129.610001           0           -1  0.0           2.397015\n",
      "2022-12-30  129.929993           0           -1  NaN                NaN\n"
     ]
    }
   ],
   "source": [
    "# Calculando a melhor média móvel com o período encontrado pelo GridSearch\n",
    "best_moving_average = data['Close'].rolling(window=best_window).mean()\n",
    "\n",
    "# Criando colunas para sinal de compra (1), sinal de venda (-1) e sinal neutro (0)\n",
    "data['Buy_Signal'] = np.where(data['Close'] > best_moving_average, 1, 0)\n",
    "data['Sell_Signal'] = np.where(data['Close'] < best_moving_average, -1, 0)\n",
    "\n",
    "# Calculando o lucro e a perda (PnL) da estratégia\n",
    "data['PnL'] = data['Close'].pct_change() * data['Buy_Signal'].shift(-1)\n",
    "\n",
    "# Calculando o retorno cumulativo\n",
    "data['Cumulative_Return'] = (1 + data['PnL']).cumprod()\n",
    "\n",
    "# Visualizando os resultados\n",
    "print(data[['Close', 'Buy_Signal', 'Sell_Signal', 'PnL', 'Cumulative_Return']].tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retorno anualizado: nan%\n",
      "Índice de Sharpe: 0.14\n",
      "Drawdown máximo: -11.03%\n"
     ]
    }
   ],
   "source": [
    "# Cálculo do retorno anualizado\n",
    "days_per_year = 252  # Supondo que há 252 dias úteis em um ano\n",
    "total_trading_days = data['PnL'].count()\n",
    "annualized_return = ((data['Cumulative_Return'].iloc[-1]) ** (days_per_year / total_trading_days)) - 1\n",
    "\n",
    "# Cálculo do risco livre (pode ser ajustado de acordo com a taxa livre de risco atual)\n",
    "risk_free_rate = 0.03  # Exemplo: 3% ao ano\n",
    "\n",
    "# Cálculo do índice de Sharpe\n",
    "daily_returns = data['PnL']\n",
    "daily_risk_free_rate = (1 + risk_free_rate) ** (1 / days_per_year) - 1\n",
    "daily_volatility = daily_returns.std()\n",
    "sharpe_ratio = (daily_returns.mean() - daily_risk_free_rate) / daily_volatility\n",
    "\n",
    "# Cálculo do drawdown máximo\n",
    "cumulative_returns = data['Cumulative_Return']\n",
    "previous_peaks = cumulative_returns.cummax()\n",
    "drawdowns = cumulative_returns / previous_peaks - 1\n",
    "max_drawdown = drawdowns.min()\n",
    "\n",
    "# Visualizando as métricas\n",
    "print(f\"Retorno anualizado: {annualized_return:.2%}\")\n",
    "print(f\"Índice de Sharpe: {sharpe_ratio:.2f}\")\n",
    "print(f\"Drawdown máximo: {max_drawdown:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo: 0.72\n",
      "Relatório de Classificação:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.93      0.62      0.74        65\n",
      "         1.0       0.57      0.92      0.70        36\n",
      "\n",
      "    accuracy                           0.72       101\n",
      "   macro avg       0.75      0.77      0.72       101\n",
      "weighted avg       0.80      0.72      0.73       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Preparação dos dados para o modelo de machine learning\n",
    "X = data[['Close']].shift(1).dropna()  # Usamos o preço de fechamento do dia anterior como feature\n",
    "y = data['Buy_Signal'].shift(-1).dropna()  # O sinal de compra do próximo dia é o rótulo\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Criação e treinamento do modelo de Regressão Logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_output = classification_report(y_test, y_pred)\n",
    "\n",
    "# Visualizando os resultados\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(\"Relatório de Classificação:\\n\", classification_report_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo SVM: 0.72\n",
      "Relatório de Classificação do modelo SVM:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.63      0.75        65\n",
      "         1.0       0.57      0.89      0.70        36\n",
      "\n",
      "    accuracy                           0.72       101\n",
      "   macro avg       0.74      0.76      0.72       101\n",
      "weighted avg       0.79      0.72      0.73       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Criação e treinamento do modelo SVM\n",
    "model_svm = SVC(kernel='linear', C=1.0)\n",
    "model_svm.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo SVM\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "classification_report_output_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "# Visualizando os resultados do modelo SVM\n",
    "print(f\"Acurácia do modelo SVM: {accuracy_svm:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo SVM:\\n\", classification_report_output_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      "Erro Médio Absoluto do modelo de Rede Neural: 1.00\n",
      "Erro Médio Quadrático do modelo de Rede Neural: 1.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo de Rede Neural\n",
    "model_nn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(1,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_nn.compile(optimizer='adam', loss='mean_squared_error')  # Usando MSE para regressão\n",
    "model_nn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_nn = model_nn.predict(X_test)\n",
    "\n",
    "# Calculando as métricas de regressão\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "\n",
    "# Visualizando os resultados do modelo de Rede Neural\n",
    "print(f\"Erro Médio Absoluto do modelo de Rede Neural: {mae_nn:.2f}\")\n",
    "print(f\"Erro Médio Quadrático do modelo de Rede Neural: {mse_nn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro Médio Absoluto do modelo ARIMA: 165.42\n",
      "Erro Médio Quadrático do modelo ARIMA: 27366.36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:471: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n",
      "  self._init_dates(dates, freq)\n",
      "c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\statsmodels\\tsa\\base\\tsa_model.py:834: ValueWarning: No supported index is available. Prediction results will be given with an integer index beginning at `start`.\n",
      "  return get_prediction_index(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo ARIMA\n",
    "model_arima = ARIMA(X_train, order=(5, 1, 0))  # Ordem (p, d, q) ajustável conforme necessário\n",
    "model_arima_fit = model_arima.fit()\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_arima = model_arima_fit.forecast(steps=len(X_test))\n",
    "\n",
    "# Calculando as métricas de regressão\n",
    "mae_arima = mean_absolute_error(y_test, y_pred_arima)\n",
    "mse_arima = mean_squared_error(y_test, y_pred_arima)\n",
    "\n",
    "# Visualizando os resultados do modelo ARIMA\n",
    "print(f\"Erro Médio Absoluto do modelo ARIMA: {mae_arima:.2f}\")\n",
    "print(f\"Erro Médio Quadrático do modelo ARIMA: {mse_arima:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 1s 4ms/step\n",
      "Erro Médio Absoluto do modelo LSTM: 1.01\n",
      "Erro Médio Quadrático do modelo LSTM: 1.01\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Preparação dos dados para o modelo LSTM\n",
    "X_train_lstm = X_train.values.reshape(-1, 1, 1)\n",
    "X_test_lstm = X_test.values.reshape(-1, 1, 1)\n",
    "\n",
    "# Criação e treinamento do modelo LSTM\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, input_shape=(1, 1)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')  # Usando MSE para regressão\n",
    "model_lstm.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_lstm = model_lstm.predict(X_test_lstm)\n",
    "\n",
    "# Calculando as métricas de regressão\n",
    "mae_lstm = mean_absolute_error(y_test, y_pred_lstm)\n",
    "mse_lstm = mean_squared_error(y_test, y_pred_lstm)\n",
    "\n",
    "# Visualizando os resultados do modelo LSTM\n",
    "print(f\"Erro Médio Absoluto do modelo LSTM: {mae_lstm:.2f}\")\n",
    "print(f\"Erro Médio Quadrático do modelo LSTM: {mse_lstm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT-repository\\github-machine-learning-trading\\machine-learning-trading\\notebook_\\start.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X55sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m             q_table[state_idx, action] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39m0.1\u001b[39m) \u001b[39m*\u001b[39m q_table[state_idx, action] \u001b[39m+\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m \u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(q_table[new_state_idx]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X55sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m             state \u001b[39m=\u001b[39m new_state\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X55sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m q_learning(env)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X55sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m# Teste da estratégia treinada\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X55sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n",
      "\u001b[1;32md:\\GIT-repository\\github-machine-learning-trading\\machine-learning-trading\\notebook_\\start.ipynb Cell 17\u001b[0m in \u001b[0;36mq_learning\u001b[1;34m(env, num_episodes)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X55sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m new_state_idx \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(new_state[\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(q_table))  \u001b[39m# Usar new_state[0] para obter o valor único do novo estado\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X55sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m \u001b[39m# Atualizar a tabela Q\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X55sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m q_table[state_idx, action] \u001b[39m=\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39m0.1\u001b[39m) \u001b[39m*\u001b[39m q_table[state_idx, action] \u001b[39m+\u001b[39m \u001b[39m0.1\u001b[39m \u001b[39m*\u001b[39m (reward \u001b[39m+\u001b[39m \u001b[39m0.9\u001b[39m \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mmax(q_table[new_state_idx]))\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X55sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m state \u001b[39m=\u001b[39m new_state\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1 is out of bounds for axis 0 with size 1"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.action_space = gym.spaces.Discrete(3)  # 0: não fazer nada, 1: comprar, 2: vender\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.profit = 0\n",
    "        self.holdings = 0\n",
    "        self.initial_balance = 1000  # Defina o saldo inicial desejado\n",
    "        self.balance = self.initial_balance\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return np.array([self.data['Close'].iloc[self.current_step] / self.data['Close'].max()])\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if action == 1:  # Comprar\n",
    "            if self.holdings == 0:\n",
    "                self.holdings = self.balance / self.data['Close'].iloc[self.current_step]\n",
    "                self.balance = 0\n",
    "\n",
    "        elif action == 2:  # Vender\n",
    "            if self.holdings > 0:\n",
    "                self.balance = self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "                self.holdings = 0\n",
    "\n",
    "        # Calcular a recompensa\n",
    "        current_balance = self.balance + self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "        self.profit = (current_balance - self.initial_balance) / self.initial_balance\n",
    "\n",
    "        # Verificar se o episódio terminou (pode adicionar outras condições de término)\n",
    "        done = self.current_step == len(self.data) - 1\n",
    "\n",
    "        # Definir a recompensa\n",
    "        if done:\n",
    "            reward = self.profit\n",
    "        else:\n",
    "            reward = self.profit - 0.001  # Penalidade diária para incentivar ações mais curtas\n",
    "\n",
    "        return self._next_observation(), reward, done, {}\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Criação do ambiente de trading\n",
    "env = TradingEnv(data)\n",
    "\n",
    "\n",
    "# Implementação do algoritmo Q-Learning (com a correção do índice do estado)\n",
    "def q_learning(env, num_episodes=100):\n",
    "    q_table = np.zeros((len(env.observation_space.high), env.action_space.n))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Converter o estado (observation) para um índice inteiro\n",
    "            state_idx = int(state[0] * len(q_table))  # Usar state[0] para obter o valor único do estado\n",
    "\n",
    "            action = np.argmax(q_table[state_idx])\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Converter o novo estado (new_state) para um índice inteiro\n",
    "            new_state_idx = int(new_state[0] * len(q_table))  # Usar new_state[0] para obter o valor único do novo estado\n",
    "\n",
    "            # Atualizar a tabela Q\n",
    "            q_table[state_idx, action] = (1 - 0.1) * q_table[state_idx, action] + 0.1 * (reward + 0.9 * np.max(q_table[new_state_idx]))\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "q_learning(env)\n",
    "\n",
    "# Teste da estratégia treinada\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "final_profit = env.profit\n",
    "print(f\"Lucro final da estratégia: {final_profit:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 2ms/step\n",
      "Erro Médio Absoluto do modelo LSTM: 2.59\n",
      "Erro Médio Quadrático do modelo LSTM: 11.67\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data[['Close']])\n",
    "X, y = [], []\n",
    "for i in range(len(data_scaled) - 1):\n",
    "    X.append(data_scaled[i])\n",
    "    y.append(data_scaled[i + 1][0])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Reshape dos dados para o formato [amostras, tempo, características]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Criação e treinamento do modelo LSTM\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, input_shape=(1, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model_lstm.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_lstm = model_lstm.predict(X_test)\n",
    "\n",
    "# Transformando as previsões para a escala original\n",
    "y_pred_lstm = scaler.inverse_transform(y_pred_lstm)\n",
    "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculando as métricas de regressão\n",
    "mae_lstm = mean_absolute_error(y_test, y_pred_lstm)\n",
    "mse_lstm = mean_squared_error(y_test, y_pred_lstm)\n",
    "\n",
    "# Visualizando os resultados do modelo LSTM\n",
    "print(f\"Erro Médio Absoluto do modelo LSTM: {mae_lstm:.2f}\")\n",
    "print(f\"Erro Médio Quadrático do modelo LSTM: {mse_lstm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4/4 [==============================] - 0s 3ms/step\n",
      "Erro Médio Absoluto do modelo CNN: 3.12\n",
      "Erro Médio Quadrático do modelo CNN: 15.16\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data[['Close']])\n",
    "X, y = [], []\n",
    "look_back = 10  # Defina o tamanho da janela temporal\n",
    "for i in range(len(data_scaled) - look_back):\n",
    "    X.append(data_scaled[i:i + look_back])\n",
    "    y.append(data_scaled[i + look_back])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo CNN\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(look_back, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model_cnn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_cnn = model_cnn.predict(X_test)\n",
    "\n",
    "# Transformando as previsões para a escala original\n",
    "y_pred_cnn = scaler.inverse_transform(y_pred_cnn)\n",
    "y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculando as métricas de regressão\n",
    "mae_cnn = mean_absolute_error(y_test, y_pred_cnn)\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "\n",
    "# Visualizando os resultados do modelo CNN\n",
    "print(f\"Erro Médio Absoluto do modelo CNN: {mae_cnn:.2f}\")\n",
    "print(f\"Erro Médio Quadrático do modelo CNN: {mse_cnn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo XGBoost: 0.47\n",
      "Relatório de Classificação do modelo XGBoost:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.54      0.33      0.41        57\n",
      "           1       0.42      0.64      0.51        44\n",
      "\n",
      "    accuracy                           0.47       101\n",
      "   macro avg       0.48      0.48      0.46       101\n",
      "weighted avg       0.49      0.47      0.45       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, 0)  # Substituir os valores -1 por 0\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo XGBoost\n",
    "model_xgb = XGBClassifier()\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo XGBoost\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "classification_report_output_xgb = classification_report(y_test, y_pred_xgb)\n",
    "\n",
    "# Visualizando os resultados do modelo XGBoost\n",
    "print(f\"Acurácia do modelo XGBoost: {accuracy_xgb:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo XGBoost:\\n\", classification_report_output_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia do modelo Random Forest: 0.45\n",
      "Relatório de Classificação do modelo Random Forest:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.51      0.37      0.43        57\n",
      "           1       0.40      0.55      0.46        44\n",
      "\n",
      "    accuracy                           0.45       101\n",
      "   macro avg       0.46      0.46      0.45       101\n",
      "weighted avg       0.46      0.45      0.44       101\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo Random Forest\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo Random Forest\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "classification_report_output_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "# Visualizando os resultados do modelo Random Forest\n",
    "print(f\"Acurácia do modelo Random Forest: {accuracy_rf:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo Random Forest:\\n\", classification_report_output_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'conv1d_1' (type Conv1D).\n    \n    Negative dimension size caused by subtracting 3 from 1 for '{{node sequential_4/conv1d_1/Conv1D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_4/conv1d_1/Conv1D/ExpandDims, sequential_4/conv1d_1/Conv1D/ExpandDims_1)' with input shapes: [?,1,1,64], [1,3,64,32].\n    \n    Call arguments received by layer 'conv1d_1' (type Conv1D):\n      • inputs=tf.Tensor(shape=(None, 1, 64), dtype=float32)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT-repository\\github-machine-learning-trading\\machine-learning-trading\\notebook_\\start.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 52>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X64sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m model_hybrid \u001b[39m=\u001b[39m Sequential([\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X64sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m     LSTM(\u001b[39m64\u001b[39m, input_shape\u001b[39m=\u001b[39m(look_back, \u001b[39m1\u001b[39m), return_sequences\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m),\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X64sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m     Conv1D(filters\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, kernel_size\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m'\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X64sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m     Dense(\u001b[39m1\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X64sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m ])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X64sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m model_hybrid\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39madam\u001b[39m\u001b[39m'\u001b[39m, loss\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbinary_crossentropy\u001b[39m\u001b[39m'\u001b[39m, metrics\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X64sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m model_hybrid\u001b[39m.\u001b[39;49mfit(X_train, y_train, epochs\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m, verbose\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X64sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39m# Realizando as previsões para o conjunto de teste\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X64sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m y_pred_hybrid \u001b[39m=\u001b[39m model_hybrid\u001b[39m.\u001b[39mpredict_classes(X_test)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mC:\\Users\\CAQUEM~1\\AppData\\Local\\Temp\\__autograph_generated_filexfqnl6ij.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1338, in train_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1322, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1303, in run_step  **\n        outputs = model.train_step(data)\n    File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1080, in train_step\n        y_pred = self(x, training=True)\n    File \"c:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n\n    ValueError: Exception encountered when calling layer 'conv1d_1' (type Conv1D).\n    \n    Negative dimension size caused by subtracting 3 from 1 for '{{node sequential_4/conv1d_1/Conv1D}} = Conv2D[T=DT_FLOAT, data_format=\"NHWC\", dilations=[1, 1, 1, 1], explicit_paddings=[], padding=\"VALID\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true](sequential_4/conv1d_1/Conv1D/ExpandDims, sequential_4/conv1d_1/Conv1D/ExpandDims_1)' with input shapes: [?,1,1,64], [1,3,64,32].\n    \n    Call arguments received by layer 'conv1d_1' (type Conv1D):\n      • inputs=tf.Tensor(shape=(None, 1, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Normalização dos dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Ajustar o tamanho do conjunto de teste para permitir a criação de sequências de tamanho 10\n",
    "look_back = 10\n",
    "num_samples_test = len(X_test)\n",
    "num_sequences = num_samples_test - look_back + 1\n",
    "X_test_new = np.zeros((num_sequences, look_back, 1))\n",
    "y_test_new = np.zeros(num_sequences)\n",
    "\n",
    "for i in range(num_sequences):\n",
    "    X_test_new[i] = X_test[i:i+look_back].reshape(look_back, 1)\n",
    "    y_test_new[i] = y_test[i+look_back-1]\n",
    "\n",
    "X_test = X_test_new\n",
    "y_test = y_test_new\n",
    "\n",
    "# Criação e treinamento do modelo híbrido LSTM + CNN\n",
    "model_hybrid = Sequential([\n",
    "    LSTM(64, input_shape=(look_back, 1), return_sequences=True),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_hybrid.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_hybrid = model_hybrid.predict_classes(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo híbrido LSTM + CNN\n",
    "accuracy_hybrid = accuracy_score(y_test, y_pred_hybrid)\n",
    "classification_report_output_hybrid = classification_report(y_test, y_pred_hybrid)\n",
    "\n",
    "# Visualizando os resultados do modelo híbrido LSTM + CNN\n",
    "print(f\"Acurácia do modelo híbrido LSTM + CNN: {accuracy_hybrid:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo híbrido LSTM + CNN:\\n\", classification_report_output_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`lr` is deprecated in Keras optimizer, please use `learning_rate` or use the legacy optimizer, e.g.,tf.keras.optimizers.legacy.Adam.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 129ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 25ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 24ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT-repository\\github-machine-learning-trading\\machine-learning-trading\\notebook_\\start.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 117>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X32sZmlsZQ%3D%3D?line=128'>129</a>\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X32sZmlsZQ%3D%3D?line=129'>130</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(agent\u001b[39m.\u001b[39mmemory) \u001b[39m>\u001b[39m batch_size:\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X32sZmlsZQ%3D%3D?line=130'>131</a>\u001b[0m         agent\u001b[39m.\u001b[39;49mreplay(batch_size)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X32sZmlsZQ%3D%3D?line=132'>133</a>\u001b[0m \u001b[39m# Teste da estratégia aprendida\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X32sZmlsZQ%3D%3D?line=133'>134</a>\u001b[0m state \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n",
      "\u001b[1;32md:\\GIT-repository\\github-machine-learning-trading\\machine-learning-trading\\notebook_\\start.ipynb Cell 24\u001b[0m in \u001b[0;36mDQNAgent.replay\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X32sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X32sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m     target \u001b[39m=\u001b[39m reward \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mamax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict(next_state)[\u001b[39m0\u001b[39m])\n\u001b[1;32m--> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X32sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m target_f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mpredict(state)\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X32sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m target_f[\u001b[39m0\u001b[39m][action] \u001b[39m=\u001b[39m target\n\u001b[0;32m    <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#X32sZmlsZQ%3D%3D?line=105'>106</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mfit(state, target_f, epochs\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py:2521\u001b[0m, in \u001b[0;36mModel.predict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   2512\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[0;32m   2513\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m   2514\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mUsing Model.predict with MultiWorkerMirroredStrategy \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2515\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mor TPUStrategy and AutoShardPolicy.FILE might lead to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2518\u001b[0m             stacklevel\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m,\n\u001b[0;32m   2519\u001b[0m         )\n\u001b[1;32m-> 2521\u001b[0m data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   2522\u001b[0m     x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   2523\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   2524\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps,\n\u001b[0;32m   2525\u001b[0m     initial_epoch\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m,\n\u001b[0;32m   2526\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[0;32m   2527\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   2528\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   2529\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   2530\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2531\u001b[0m     steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution,\n\u001b[0;32m   2532\u001b[0m )\n\u001b[0;32m   2534\u001b[0m \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   2535\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1678\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1676\u001b[0m         \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorExactEvalDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1677\u001b[0m     \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1678\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1285\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute, pss_evaluation_shards)\u001b[0m\n\u001b[0;32m   1282\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution \u001b[39m=\u001b[39m steps_per_execution\n\u001b[0;32m   1284\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1285\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1286\u001b[0m     x,\n\u001b[0;32m   1287\u001b[0m     y,\n\u001b[0;32m   1288\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1289\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1290\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1291\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1292\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1293\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1294\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1295\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1296\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdistribute\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1297\u001b[0m     model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m   1298\u001b[0m     pss_evaluation_shards\u001b[39m=\u001b[39;49mpss_evaluation_shards,\n\u001b[0;32m   1299\u001b[0m )\n\u001b[0;32m   1301\u001b[0m strategy \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdistribute\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1303\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:355\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    351\u001b[0m     \u001b[39mreturn\u001b[39;00m flat_dataset\n\u001b[0;32m    353\u001b[0m indices_dataset \u001b[39m=\u001b[39m indices_dataset\u001b[39m.\u001b[39mflat_map(slice_batch_indices)\n\u001b[1;32m--> 355\u001b[0m dataset \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mslice_inputs(indices_dataset, inputs)\n\u001b[0;32m    357\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mshuffle_batch\u001b[39m(\u001b[39m*\u001b[39mbatch):\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\data_adapter.py:396\u001b[0m, in \u001b[0;36mTensorLikeDataAdapter.slice_inputs\u001b[1;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgrab_batch\u001b[39m(i, data):\n\u001b[0;32m    392\u001b[0m     \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mmap_structure(\n\u001b[0;32m    393\u001b[0m         \u001b[39mlambda\u001b[39;00m d: tf\u001b[39m.\u001b[39mgather(d, i, axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m), data\n\u001b[0;32m    394\u001b[0m     )\n\u001b[1;32m--> 396\u001b[0m dataset \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39;49mmap(grab_batch, num_parallel_calls\u001b[39m=\u001b[39;49mtf\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mAUTOTUNE)\n\u001b[0;32m    398\u001b[0m \u001b[39m# Default optimizations are disabled to avoid the overhead of\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39m# (unnecessary) input pipeline graph serialization and deserialization\u001b[39;00m\n\u001b[0;32m    400\u001b[0m options \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mOptions()\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py:2278\u001b[0m, in \u001b[0;36mDatasetV2.map\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2274\u001b[0m \u001b[39m# Loaded lazily due to a circular dependency (dataset_ops -> map_op ->\u001b[39;00m\n\u001b[0;32m   2275\u001b[0m \u001b[39m# dataset_ops).\u001b[39;00m\n\u001b[0;32m   2276\u001b[0m \u001b[39m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[0;32m   2277\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mops\u001b[39;00m \u001b[39mimport\u001b[39;00m map_op\n\u001b[1;32m-> 2278\u001b[0m \u001b[39mreturn\u001b[39;00m map_op\u001b[39m.\u001b[39;49m_map_v2(\n\u001b[0;32m   2279\u001b[0m     \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   2280\u001b[0m     map_func,\n\u001b[0;32m   2281\u001b[0m     num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[0;32m   2282\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[0;32m   2283\u001b[0m     name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:40\u001b[0m, in \u001b[0;36m_map_v2\u001b[1;34m(input_dataset, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m     37\u001b[0m   \u001b[39mreturn\u001b[39;00m _MapDataset(\n\u001b[0;32m     38\u001b[0m       input_dataset, map_func, preserve_cardinality\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m     39\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 40\u001b[0m   \u001b[39mreturn\u001b[39;00m _ParallelMapDataset(\n\u001b[0;32m     41\u001b[0m       input_dataset,\n\u001b[0;32m     42\u001b[0m       map_func,\n\u001b[0;32m     43\u001b[0m       num_parallel_calls\u001b[39m=\u001b[39;49mnum_parallel_calls,\n\u001b[0;32m     44\u001b[0m       deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[0;32m     45\u001b[0m       preserve_cardinality\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[0;32m     46\u001b[0m       name\u001b[39m=\u001b[39;49mname)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\map_op.py:148\u001b[0m, in \u001b[0;36m_ParallelMapDataset.__init__\u001b[1;34m(self, input_dataset, map_func, num_parallel_calls, deterministic, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_dataset \u001b[39m=\u001b[39m input_dataset\n\u001b[0;32m    147\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_use_inter_op_parallelism \u001b[39m=\u001b[39m use_inter_op_parallelism\n\u001b[1;32m--> 148\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_map_func \u001b[39m=\u001b[39m structured_function\u001b[39m.\u001b[39;49mStructuredFunctionWrapper(\n\u001b[0;32m    149\u001b[0m     map_func,\n\u001b[0;32m    150\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_transformation_name(),\n\u001b[0;32m    151\u001b[0m     dataset\u001b[39m=\u001b[39;49minput_dataset,\n\u001b[0;32m    152\u001b[0m     use_legacy_function\u001b[39m=\u001b[39;49muse_legacy_function)\n\u001b[0;32m    153\u001b[0m \u001b[39mif\u001b[39;00m deterministic \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_deterministic \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mdefault\u001b[39m\u001b[39m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\data\\ops\\structured_function.py:272\u001b[0m, in \u001b[0;36mStructuredFunctionWrapper.__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m    265\u001b[0m       warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    266\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mEven though the `tf.config.experimental_run_functions_eagerly` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    267\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39moption is set, this option does not apply to tf.data functions. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    268\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39mTo force eager execution of tf.data functions, please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    269\u001b[0m           \u001b[39m\"\u001b[39m\u001b[39m`tf.data.experimental.enable_debug_mode()`.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    270\u001b[0m     fn_factory \u001b[39m=\u001b[39m trace_tf_function(defun_kwargs)\n\u001b[1;32m--> 272\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function \u001b[39m=\u001b[39m fn_factory()\n\u001b[0;32m    273\u001b[0m \u001b[39m# There is no graph to add in eager mode.\u001b[39;00m\n\u001b[0;32m    274\u001b[0m add_to_graph \u001b[39m&\u001b[39m\u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m context\u001b[39m.\u001b[39mexecuting_eagerly()\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1189\u001b[0m, in \u001b[0;36mFunction.get_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1187\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_concrete_function\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m   1188\u001b[0m   \u001b[39m# Implements GenericFunction.get_concrete_function.\u001b[39;00m\n\u001b[1;32m-> 1189\u001b[0m   concrete \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_concrete_function_garbage_collected(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1190\u001b[0m   concrete\u001b[39m.\u001b[39m_garbage_collector\u001b[39m.\u001b[39mrelease()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m   1191\u001b[0m   \u001b[39mreturn\u001b[39;00m concrete\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:1169\u001b[0m, in \u001b[0;36mFunction._get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1167\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   1168\u001b[0m     initializers \u001b[39m=\u001b[39m []\n\u001b[1;32m-> 1169\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize(args, kwargs, add_initializers_to\u001b[39m=\u001b[39;49minitializers)\n\u001b[0;32m   1170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_initialize_uninitialized_variables(initializers)\n\u001b[0;32m   1172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables:\n\u001b[0;32m   1173\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m   1174\u001b[0m   \u001b[39m# version which is guaranteed to never create variables.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:694\u001b[0m, in \u001b[0;36mFunction._initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    691\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn\u001b[39m.\u001b[39m_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_name  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    692\u001b[0m \u001b[39m# Force the definition of the function for these arguments\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_concrete_variable_creation_fn \u001b[39m=\u001b[39m (\n\u001b[1;32m--> 694\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn    \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    695\u001b[0m     \u001b[39m.\u001b[39m_get_concrete_function_internal_garbage_collected(\n\u001b[0;32m    696\u001b[0m         \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds))\n\u001b[0;32m    698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minvalid_creator_scope\u001b[39m(\u001b[39m*\u001b[39munused_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39munused_kwds):\n\u001b[0;32m    699\u001b[0m   \u001b[39m\"\"\"Disables variable creation.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:176\u001b[0m, in \u001b[0;36mTracingCompiler._get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[39m\"\"\"Returns a concrete function which cleans up its graph function.\"\"\"\u001b[39;00m\n\u001b[0;32m    175\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m--> 176\u001b[0m   concrete_function, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_concrete_function(args, kwargs)\n\u001b[0;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:171\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_concrete_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m   args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_signature\n\u001b[0;32m    169\u001b[0m   kwargs \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compiler.py:393\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m    390\u001b[0m placeholder_context \u001b[39m=\u001b[39m trace_type\u001b[39m.\u001b[39mInternalPlaceholderContext(\n\u001b[0;32m    391\u001b[0m     func_graph, placeholder_mapping)\n\u001b[0;32m    392\u001b[0m \u001b[39mwith\u001b[39;00m func_graph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m--> 393\u001b[0m   placeholder_bound_args \u001b[39m=\u001b[39m target_func_type\u001b[39m.\u001b[39;49mplaceholder_arguments(\n\u001b[0;32m    394\u001b[0m       placeholder_context)\n\u001b[0;32m    395\u001b[0m args \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39margs\n\u001b[0;32m    396\u001b[0m kwargs \u001b[39m=\u001b[39m placeholder_bound_args\u001b[39m.\u001b[39mkwargs\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\core\\function\\polymorphism\\function_type.py:323\u001b[0m, in \u001b[0;36mFunctionType.placeholder_arguments\u001b[1;34m(self, placeholder_context)\u001b[0m\n\u001b[0;32m    320\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCan not generate placeholder value for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    321\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mpartially defined function type.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    322\u001b[0m   placeholder_context\u001b[39m.\u001b[39mupdate_naming_scope(parameter\u001b[39m.\u001b[39mname)\n\u001b[1;32m--> 323\u001b[0m   arguments[parameter\u001b[39m.\u001b[39mname] \u001b[39m=\u001b[39m parameter\u001b[39m.\u001b[39;49mtype_constraint\u001b[39m.\u001b[39;49mplaceholder_value(\n\u001b[0;32m    324\u001b[0m       placeholder_context)\n\u001b[0;32m    326\u001b[0m \u001b[39mreturn\u001b[39;00m inspect\u001b[39m.\u001b[39mBoundArguments(\u001b[39mself\u001b[39m, arguments)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:271\u001b[0m, in \u001b[0;36mTensorSpec.placeholder_value\u001b[1;34m(self, placeholder_context)\u001b[0m\n\u001b[0;32m    269\u001b[0m     placeholder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_graph_placeholder(context_graph, name\u001b[39m=\u001b[39mname)\n\u001b[0;32m    270\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 271\u001b[0m   placeholder \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_graph_placeholder(context_graph, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    273\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    274\u001b[0m   \u001b[39m# Record the requested/user-specified name in case it's different than\u001b[39;00m\n\u001b[0;32m    275\u001b[0m   \u001b[39m# the uniquified name, for validation when exporting signatures.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m   placeholder\u001b[39m.\u001b[39mop\u001b[39m.\u001b[39m_set_attr(  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m    277\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39m_user_specified_name\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    278\u001b[0m       attr_value_pb2\u001b[39m.\u001b[39mAttrValue(s\u001b[39m=\u001b[39mcompat\u001b[39m.\u001b[39mas_bytes(name)))\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\tensor.py:309\u001b[0m, in \u001b[0;36mTensorSpec._graph_placeholder\u001b[1;34m(self, graph, name)\u001b[0m\n\u001b[0;32m    307\u001b[0m attrs \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m: dtype_value, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m: shape}\n\u001b[0;32m    308\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 309\u001b[0m   op \u001b[39m=\u001b[39m graph\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    310\u001b[0m       \u001b[39m\"\u001b[39;49m\u001b[39mPlaceholder\u001b[39;49m\u001b[39m\"\u001b[39;49m, [], [dtype], input_types\u001b[39m=\u001b[39;49m[],\n\u001b[0;32m    311\u001b[0m       attrs\u001b[39m=\u001b[39;49mattrs, name\u001b[39m=\u001b[39;49mname)\n\u001b[0;32m    312\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    313\u001b[0m   \u001b[39m# TODO(b/262413656) Sometimes parameter names are not valid op names, in\u001b[39;00m\n\u001b[0;32m    314\u001b[0m   \u001b[39m# which case an unnamed placeholder is created instead. Update this logic\u001b[39;00m\n\u001b[0;32m    315\u001b[0m   \u001b[39m# to sanitize the name instead of falling back on unnamed placeholders.\u001b[39;00m\n\u001b[0;32m    316\u001b[0m   logging\u001b[39m.\u001b[39mwarning(e)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py:670\u001b[0m, in \u001b[0;36mFuncGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    668\u001b[0m   inp \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcapture(inp)\n\u001b[0;32m    669\u001b[0m   captured_inputs\u001b[39m.\u001b[39mappend(inp)\n\u001b[1;32m--> 670\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m_create_op_internal(  \u001b[39m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    671\u001b[0m     op_type, captured_inputs, dtypes, input_types, name, attrs, op_def,\n\u001b[0;32m    672\u001b[0m     compute_device)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:3381\u001b[0m, in \u001b[0;36mGraph._create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3378\u001b[0m \u001b[39m# _create_op_helper mutates the new Operation. `_mutation_lock` ensures a\u001b[39;00m\n\u001b[0;32m   3379\u001b[0m \u001b[39m# Session.run call cannot occur between creating and mutating the op.\u001b[39;00m\n\u001b[0;32m   3380\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_mutation_lock():\n\u001b[1;32m-> 3381\u001b[0m   ret \u001b[39m=\u001b[39m Operation\u001b[39m.\u001b[39;49mfrom_node_def(\n\u001b[0;32m   3382\u001b[0m       node_def,\n\u001b[0;32m   3383\u001b[0m       \u001b[39mself\u001b[39;49m,\n\u001b[0;32m   3384\u001b[0m       inputs\u001b[39m=\u001b[39;49minputs,\n\u001b[0;32m   3385\u001b[0m       output_types\u001b[39m=\u001b[39;49mdtypes,\n\u001b[0;32m   3386\u001b[0m       control_inputs\u001b[39m=\u001b[39;49mcontrol_inputs,\n\u001b[0;32m   3387\u001b[0m       input_types\u001b[39m=\u001b[39;49minput_types,\n\u001b[0;32m   3388\u001b[0m       original_op\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_default_original_op,\n\u001b[0;32m   3389\u001b[0m       op_def\u001b[39m=\u001b[39;49mop_def,\n\u001b[0;32m   3390\u001b[0m   )\n\u001b[0;32m   3391\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_op_helper(ret, compute_device\u001b[39m=\u001b[39mcompute_device)\n\u001b[0;32m   3392\u001b[0m \u001b[39mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1889\u001b[0m, in \u001b[0;36mOperation.from_node_def\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   1886\u001b[0m     control_input_ops\u001b[39m.\u001b[39mappend(control_op)\n\u001b[0;32m   1888\u001b[0m \u001b[39m# Initialize c_op from node_def and other inputs\u001b[39;00m\n\u001b[1;32m-> 1889\u001b[0m c_op \u001b[39m=\u001b[39m _create_c_op(g, node_def, inputs, control_input_ops, op_def\u001b[39m=\u001b[39;49mop_def)\n\u001b[0;32m   1890\u001b[0m \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m Operation(c_op, GraphTensor)\n\u001b[0;32m   1891\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_init(g)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py:1748\u001b[0m, in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs, op_def, extract_traceback)\u001b[0m\n\u001b[0;32m   1744\u001b[0m   pywrap_tf_session\u001b[39m.\u001b[39mTF_SetAttrValueProto(op_desc, compat\u001b[39m.\u001b[39mas_str(name),\n\u001b[0;32m   1745\u001b[0m                                          serialized)\n\u001b[0;32m   1747\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1748\u001b[0m   c_op \u001b[39m=\u001b[39m pywrap_tf_session\u001b[39m.\u001b[39;49mTF_FinishOperation(op_desc)\n\u001b[0;32m   1749\u001b[0m \u001b[39mexcept\u001b[39;00m errors\u001b[39m.\u001b[39mInvalidArgumentError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m   1750\u001b[0m   \u001b[39m# Convert to ValueError for backwards compatibility.\u001b[39;00m\n\u001b[0;32m   1751\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(e\u001b[39m.\u001b[39mmessage)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.action_space = spaces.Discrete(3)  # 0: não fazer nada, 1: comprar, 2: vender\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.profit = 0\n",
    "        self.holdings = 0\n",
    "        self.initial_balance = 1000  # Defina o saldo inicial desejado\n",
    "        self.balance = self.initial_balance\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return np.array([self.data['Close'].iloc[self.current_step] / self.data['Close'].max()])\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if action == 1:  # Comprar\n",
    "            if self.holdings == 0:\n",
    "                self.holdings = self.balance / self.data['Close'].iloc[self.current_step]\n",
    "                self.balance = 0\n",
    "\n",
    "        elif action == 2:  # Vender\n",
    "            if self.holdings > 0:\n",
    "                self.balance = self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "                self.holdings = 0\n",
    "\n",
    "        # Calcular a recompensa\n",
    "        current_balance = self.balance + self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "        self.profit = (current_balance - self.initial_balance) / self.initial_balance\n",
    "\n",
    "        # Verificar se o episódio terminou (pode adicionar outras condições de término)\n",
    "        done = self.current_step == len(self.data) - 1\n",
    "\n",
    "        # Definir a recompensa\n",
    "        if done:\n",
    "            reward = self.profit\n",
    "        else:\n",
    "            reward = self.profit - 0.001  # Penalidade diária para incentivar ações mais curtas\n",
    "\n",
    "        return self._next_observation(), reward, done, {}\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Criação do ambiente de trading\n",
    "env = TradingEnv(data)\n",
    "\n",
    "# Implementação do algoritmo Deep Q-Network (DQN)\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Fator de desconto\n",
    "        self.epsilon = 1.0  # Probabilidade de exploração inicial\n",
    "        self.epsilon_decay = 0.995  # Taxa de decaimento da probabilidade de exploração\n",
    "        self.epsilon_min = 0.01  # Probabilidade mínima de exploração\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Configuração do agente DQN\n",
    "state_size = 1\n",
    "action_size = 3\n",
    "batch_size = 32\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Treinamento do agente DQN\n",
    "for e in range(100):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"Época: {}/{}, Lucro: {:.2f}\"\n",
    "                  .format(e, 100, env.profit))\n",
    "            break\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "# Teste da estratégia aprendida\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    next_state, _, done, _ = env.step(action)\n",
    "    state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "final_profit = env.profit\n",
    "print(f\"Lucro final da estratégia: {final_profit:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
