{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'data_'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32md:\\GIT-repository\\github-machine-learning-trading\\machine-learning-trading\\notebook_\\start.ipynb Cell 1\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#W0sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m data \u001b[39m=\u001b[39m yf\u001b[39m.\u001b[39mdownload(\u001b[39m'\u001b[39m\u001b[39mAAPL\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#W0sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m                    start\u001b[39m=\u001b[39mstart,\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m                    end\u001b[39m=\u001b[39mend)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m data\u001b[39m.\u001b[39mhead()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/GIT-repository/github-machine-learning-trading/machine-learning-trading/notebook_/start.ipynb#W0sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m data\u001b[39m.\u001b[39;49mto_csv(\u001b[39m'\u001b[39;49m\u001b[39mdata_/AAPL.csv\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3540\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3542\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3543\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3544\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3548\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3549\u001b[0m )\n\u001b[1;32m-> 3551\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3552\u001b[0m     path_or_buf,\n\u001b[0;32m   3553\u001b[0m     line_terminator\u001b[39m=\u001b[39;49mline_terminator,\n\u001b[0;32m   3554\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3555\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3556\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3557\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3558\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3559\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3560\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3561\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3562\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3563\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3564\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3565\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3566\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3567\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3568\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[39m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1183\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:694\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    692\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    693\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 694\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    696\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    697\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    698\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Caíque Miranda\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:568\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    566\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    567\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 568\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'data_'"
     ]
    }
   ],
   "source": [
    "# dados fin\n",
    "import yfinance as yf\n",
    "\n",
    "start = '2021-01-01'\n",
    "end = '2023-01-01'\n",
    "\n",
    "data = yf.download('AAPL', \n",
    "                   start=start,\n",
    "                   end=end)\n",
    "\n",
    "data.head()\n",
    "\n",
    "data.to_csv('data_/AAPL.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Carregando os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_moving_average(data, window):\n",
    "    return data['Close'].rolling(window=window).mean()\n",
    "\n",
    "# Vamos testar para diferentes períodos de média móvel (por exemplo, de 5 a 50 dias)\n",
    "moving_averages = []\n",
    "for window in range(5, 51):\n",
    "    moving_averages.append(calculate_moving_average(data, window))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_from_moving_average(data, moving_average):\n",
    "    return abs(data['Close'] - moving_average)\n",
    "\n",
    "distance_from_moving_averages = []\n",
    "for moving_average in moving_averages:\n",
    "    distance_from_moving_averages.append(calculate_distance_from_moving_average(data, moving_average))\n",
    "\n",
    "# Vamos calcular a média das distâncias para cada período\n",
    "mean_distances = [dist.mean() for dist in distance_from_moving_averages]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_window = mean_distances.index(min(mean_distances)) + 5  # +5 pois começamos a partir da janela 5\n",
    "\n",
    "print(f'A melhor média móvel é de {best_window} dias.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Carregando os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Definindo a função para calcular a distância da média móvel simples\n",
    "def calculate_distance_from_simple_moving_average(data, window):\n",
    "    moving_average = data['Close'].rolling(window=window).mean()\n",
    "    return abs(data['Close'] - moving_average)\n",
    "\n",
    "# Criação de um array de períodos de média móvel a serem testados\n",
    "window_sizes = np.arange(5, 51)\n",
    "\n",
    "# Preparação dos dados para validação cruzada\n",
    "X = data[['Close']]\n",
    "y = data['Close']\n",
    "\n",
    "# Definindo o modelo de regressão linear\n",
    "model = LinearRegression()\n",
    "\n",
    "# Inicializando as listas para armazenar os resultados\n",
    "best_window = None\n",
    "best_mse = float('inf')\n",
    "mse_values = []\n",
    "\n",
    "# Realizando a busca pelo melhor período de média móvel\n",
    "for window in window_sizes:\n",
    "    mse = -np.mean(calculate_distance_from_simple_moving_average(data, window)**2)\n",
    "    mse_values.append(mse)\n",
    "    if mse < best_mse:\n",
    "        best_mse = mse\n",
    "        best_window = window\n",
    "\n",
    "print(f\"A melhor média móvel é de {best_window} dias, com MSE de {best_mse:.2f}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculando a melhor média móvel com o período encontrado pelo GridSearch\n",
    "best_moving_average = data['Close'].rolling(window=best_window).mean()\n",
    "\n",
    "# Criando colunas para sinal de compra (1), sinal de venda (-1) e sinal neutro (0)\n",
    "data['Buy_Signal'] = np.where(data['Close'] > best_moving_average, 1, 0)\n",
    "data['Sell_Signal'] = np.where(data['Close'] < best_moving_average, -1, 0)\n",
    "\n",
    "# Calculando o lucro e a perda (PnL) da estratégia\n",
    "data['PnL'] = data['Close'].pct_change() * data['Buy_Signal'].shift(-1)\n",
    "\n",
    "# Calculando o retorno cumulativo\n",
    "data['Cumulative_Return'] = (1 + data['PnL']).cumprod()\n",
    "\n",
    "# Visualizando os resultados\n",
    "print(data[['Close', 'Buy_Signal', 'Sell_Signal', 'PnL', 'Cumulative_Return']].tail(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cálculo do retorno anualizado\n",
    "days_per_year = 252  # Supondo que há 252 dias úteis em um ano\n",
    "total_trading_days = data['PnL'].count()\n",
    "annualized_return = ((data['Cumulative_Return'].iloc[-1]) ** (days_per_year / total_trading_days)) - 1\n",
    "\n",
    "# Cálculo do risco livre (pode ser ajustado de acordo com a taxa livre de risco atual)\n",
    "risk_free_rate = 0.03  # Exemplo: 3% ao ano\n",
    "\n",
    "# Cálculo do índice de Sharpe\n",
    "daily_returns = data['PnL']\n",
    "daily_risk_free_rate = (1 + risk_free_rate) ** (1 / days_per_year) - 1\n",
    "daily_volatility = daily_returns.std()\n",
    "sharpe_ratio = (daily_returns.mean() - daily_risk_free_rate) / daily_volatility\n",
    "\n",
    "# Cálculo do drawdown máximo\n",
    "cumulative_returns = data['Cumulative_Return']\n",
    "previous_peaks = cumulative_returns.cummax()\n",
    "drawdowns = cumulative_returns / previous_peaks - 1\n",
    "max_drawdown = drawdowns.min()\n",
    "\n",
    "# Visualizando as métricas\n",
    "print(f\"Retorno anualizado: {annualized_return:.2%}\")\n",
    "print(f\"Índice de Sharpe: {sharpe_ratio:.2f}\")\n",
    "print(f\"Drawdown máximo: {max_drawdown:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Preparação dos dados para o modelo de machine learning\n",
    "X = data[['Close']].shift(1).dropna()  # Usamos o preço de fechamento do dia anterior como feature\n",
    "y = data['Buy_Signal'].shift(-1).dropna()  # O sinal de compra do próximo dia é o rótulo\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_test = X[:train_size], X[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "# Criação e treinamento do modelo de Regressão Logística\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_report_output = classification_report(y_test, y_pred)\n",
    "\n",
    "# Visualizando os resultados\n",
    "print(f\"Acurácia do modelo: {accuracy:.2f}\")\n",
    "print(\"Relatório de Classificação:\\n\", classification_report_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Criação e treinamento do modelo SVM\n",
    "model_svm = SVC(kernel='linear', C=1.0)\n",
    "model_svm.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_svm = model_svm.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo SVM\n",
    "accuracy_svm = accuracy_score(y_test, y_pred_svm)\n",
    "classification_report_output_svm = classification_report(y_test, y_pred_svm)\n",
    "\n",
    "# Visualizando os resultados do modelo SVM\n",
    "print(f\"Acurácia do modelo SVM: {accuracy_svm:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo SVM:\\n\", classification_report_output_svm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo de Rede Neural\n",
    "model_nn = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(1,)),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_nn.compile(optimizer='adam', loss='mean_squared_error')  # Usando MSE para regressão\n",
    "model_nn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_nn = model_nn.predict(X_test)\n",
    "\n",
    "# Calculando as métricas de regressão\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
    "\n",
    "# Visualizando os resultados do modelo de Rede Neural\n",
    "print(f\"Erro Médio Absoluto do modelo de Rede Neural: {mae_nn:.2f}\")\n",
    "print(f\"Erro Médio Quadrático do modelo de Rede Neural: {mse_nn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo ARIMA\n",
    "model_arima = ARIMA(X_train, order=(5, 1, 0))  # Ordem (p, d, q) ajustável conforme necessário\n",
    "model_arima_fit = model_arima.fit()\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_arima = model_arima_fit.forecast(steps=len(X_test))\n",
    "\n",
    "# Calculando as métricas de regressão\n",
    "mae_arima = mean_absolute_error(y_test, y_pred_arima)\n",
    "mse_arima = mean_squared_error(y_test, y_pred_arima)\n",
    "\n",
    "# Visualizando os resultados do modelo ARIMA\n",
    "print(f\"Erro Médio Absoluto do modelo ARIMA: {mae_arima:.2f}\")\n",
    "print(f\"Erro Médio Quadrático do modelo ARIMA: {mse_arima:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Preparação dos dados para o modelo LSTM\n",
    "X_train_lstm = X_train.values.reshape(-1, 1, 1)\n",
    "X_test_lstm = X_test.values.reshape(-1, 1, 1)\n",
    "\n",
    "# Criação e treinamento do modelo LSTM\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, input_shape=(1, 1)),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')  # Usando MSE para regressão\n",
    "model_lstm.fit(X_train_lstm, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_lstm = model_lstm.predict(X_test_lstm)\n",
    "\n",
    "# Calculando as métricas de regressão\n",
    "mae_lstm = mean_absolute_error(y_test, y_pred_lstm)\n",
    "mse_lstm = mean_squared_error(y_test, y_pred_lstm)\n",
    "\n",
    "# Visualizando os resultados do modelo LSTM\n",
    "print(f\"Erro Médio Absoluto do modelo LSTM: {mae_lstm:.2f}\")\n",
    "print(f\"Erro Médio Quadrático do modelo LSTM: {mse_lstm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.action_space = gym.spaces.Discrete(3)  # 0: não fazer nada, 1: comprar, 2: vender\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.profit = 0\n",
    "        self.holdings = 0\n",
    "        self.initial_balance = 1000  # Defina o saldo inicial desejado\n",
    "        self.balance = self.initial_balance\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return np.array([self.data['Close'].iloc[self.current_step] / self.data['Close'].max()])\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if action == 1:  # Comprar\n",
    "            if self.holdings == 0:\n",
    "                self.holdings = self.balance / self.data['Close'].iloc[self.current_step]\n",
    "                self.balance = 0\n",
    "\n",
    "        elif action == 2:  # Vender\n",
    "            if self.holdings > 0:\n",
    "                self.balance = self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "                self.holdings = 0\n",
    "\n",
    "        # Calcular a recompensa\n",
    "        current_balance = self.balance + self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "        self.profit = (current_balance - self.initial_balance) / self.initial_balance\n",
    "\n",
    "        # Verificar se o episódio terminou (pode adicionar outras condições de término)\n",
    "        done = self.current_step == len(self.data) - 1\n",
    "\n",
    "        # Definir a recompensa\n",
    "        if done:\n",
    "            reward = self.profit\n",
    "        else:\n",
    "            reward = self.profit - 0.001  # Penalidade diária para incentivar ações mais curtas\n",
    "\n",
    "        return self._next_observation(), reward, done, {}\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Criação do ambiente de trading\n",
    "env = TradingEnv(data)\n",
    "\n",
    "\n",
    "# Implementação do algoritmo Q-Learning (com a correção do índice do estado)\n",
    "def q_learning(env, num_episodes=100):\n",
    "    q_table = np.zeros((len(env.observation_space.high), env.action_space.n))\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            # Converter o estado (observation) para um índice inteiro\n",
    "            state_idx = int(state[0] * len(q_table))  # Usar state[0] para obter o valor único do estado\n",
    "\n",
    "            action = np.argmax(q_table[state_idx])\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Converter o novo estado (new_state) para um índice inteiro\n",
    "            new_state_idx = int(new_state[0] * len(q_table))  # Usar new_state[0] para obter o valor único do novo estado\n",
    "\n",
    "            # Atualizar a tabela Q\n",
    "            q_table[state_idx, action] = (1 - 0.1) * q_table[state_idx, action] + 0.1 * (reward + 0.9 * np.max(q_table[new_state_idx]))\n",
    "\n",
    "            state = new_state\n",
    "\n",
    "q_learning(env)\n",
    "\n",
    "# Teste da estratégia treinada\n",
    "state = env.reset()\n",
    "done = False\n",
    "\n",
    "while not done:\n",
    "    action = np.argmax(q_table[state])\n",
    "    state, _, done, _ = env.step(action)\n",
    "\n",
    "final_profit = env.profit\n",
    "print(f\"Lucro final da estratégia: {final_profit:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data[['Close']])\n",
    "X, y = [], []\n",
    "for i in range(len(data_scaled) - 1):\n",
    "    X.append(data_scaled[i])\n",
    "    y.append(data_scaled[i + 1][0])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Reshape dos dados para o formato [amostras, tempo, características]\n",
    "X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])\n",
    "X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "# Criação e treinamento do modelo LSTM\n",
    "model_lstm = Sequential([\n",
    "    LSTM(64, input_shape=(1, 1)),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_lstm.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model_lstm.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_lstm = model_lstm.predict(X_test)\n",
    "\n",
    "# Transformando as previsões para a escala original\n",
    "y_pred_lstm = scaler.inverse_transform(y_pred_lstm)\n",
    "y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Calculando as métricas de regressão\n",
    "mae_lstm = mean_absolute_error(y_test, y_pred_lstm)\n",
    "mse_lstm = mean_squared_error(y_test, y_pred_lstm)\n",
    "\n",
    "# Visualizando os resultados do modelo LSTM\n",
    "print(f\"Erro Médio Absoluto do modelo LSTM: {mae_lstm:.2f}\")\n",
    "print(f\"Erro Médio Quadrático do modelo LSTM: {mse_lstm:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Pré-processamento dos dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "data_scaled = scaler.fit_transform(data[['Close']])\n",
    "X, y = [], []\n",
    "look_back = 10  # Defina o tamanho da janela temporal\n",
    "for i in range(len(data_scaled) - look_back):\n",
    "    X.append(data_scaled[i:i + look_back])\n",
    "    y.append(data_scaled[i + look_back])\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo CNN\n",
    "model_cnn = Sequential([\n",
    "    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(look_back, 1)),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model_cnn.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_cnn = model_cnn.predict(X_test)\n",
    "\n",
    "# Transformando as previsões para a escala original\n",
    "y_pred_cnn = scaler.inverse_transform(y_pred_cnn)\n",
    "y_test = scaler.inverse_transform(y_test)\n",
    "\n",
    "# Calculando as métricas de regressão\n",
    "mae_cnn = mean_absolute_error(y_test, y_pred_cnn)\n",
    "mse_cnn = mean_squared_error(y_test, y_pred_cnn)\n",
    "\n",
    "# Visualizando os resultados do modelo CNN\n",
    "print(f\"Erro Médio Absoluto do modelo CNN: {mae_cnn:.2f}\")\n",
    "print(f\"Erro Médio Quadrático do modelo CNN: {mse_cnn:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, 0)  # Substituir os valores -1 por 0\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo XGBoost\n",
    "model_xgb = XGBClassifier()\n",
    "model_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_xgb = model_xgb.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo XGBoost\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "classification_report_output_xgb = classification_report(y_test, y_pred_xgb)\n",
    "\n",
    "# Visualizando os resultados do modelo XGBoost\n",
    "print(f\"Acurácia do modelo XGBoost: {accuracy_xgb:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo XGBoost:\\n\", classification_report_output_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Criação e treinamento do modelo Random Forest\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo Random Forest\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "classification_report_output_rf = classification_report(y_test, y_pred_rf)\n",
    "\n",
    "# Visualizando os resultados do modelo Random Forest\n",
    "print(f\"Acurácia do modelo Random Forest: {accuracy_rf:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo Random Forest:\\n\", classification_report_output_rf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, MaxPooling1D, Flatten, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Preparação dos dados\n",
    "data['Target'] = np.where(data['Close'].shift(-1) > data['Close'], 1, -1)\n",
    "\n",
    "X = data[['Close']]\n",
    "y = data['Target']\n",
    "\n",
    "# Normalização dos dados\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Divisão dos dados em conjuntos de treinamento e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Ajustar o tamanho do conjunto de teste para permitir a criação de sequências de tamanho 10\n",
    "look_back = 10\n",
    "num_samples_test = len(X_test)\n",
    "num_sequences = num_samples_test - look_back + 1\n",
    "X_test_new = np.zeros((num_sequences, look_back, 1))\n",
    "y_test_new = np.zeros(num_sequences)\n",
    "\n",
    "for i in range(num_sequences):\n",
    "    X_test_new[i] = X_test[i:i+look_back].reshape(look_back, 1)\n",
    "    y_test_new[i] = y_test[i+look_back-1]\n",
    "\n",
    "X_test = X_test_new\n",
    "y_test = y_test_new\n",
    "\n",
    "# Criação e treinamento do modelo híbrido LSTM + CNN\n",
    "model_hybrid = Sequential([\n",
    "    LSTM(64, input_shape=(look_back, 1), return_sequences=True),\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_hybrid.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_hybrid.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Realizando as previsões para o conjunto de teste\n",
    "y_pred_hybrid = model_hybrid.predict_classes(X_test)\n",
    "\n",
    "# Avaliando o desempenho do modelo híbrido LSTM + CNN\n",
    "accuracy_hybrid = accuracy_score(y_test, y_pred_hybrid)\n",
    "classification_report_output_hybrid = classification_report(y_test, y_pred_hybrid)\n",
    "\n",
    "# Visualizando os resultados do modelo híbrido LSTM + CNN\n",
    "print(f\"Acurácia do modelo híbrido LSTM + CNN: {accuracy_hybrid:.2f}\")\n",
    "print(\"Relatório de Classificação do modelo híbrido LSTM + CNN:\\n\", classification_report_output_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gym\n",
    "from gym import spaces\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "    def __init__(self, data):\n",
    "        super(TradingEnv, self).__init__()\n",
    "        self.data = data\n",
    "        self.action_space = spaces.Discrete(3)  # 0: não fazer nada, 1: comprar, 2: vender\n",
    "        self.observation_space = spaces.Box(low=0, high=1, shape=(1,), dtype=np.float32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.profit = 0\n",
    "        self.holdings = 0\n",
    "        self.initial_balance = 1000  # Defina o saldo inicial desejado\n",
    "        self.balance = self.initial_balance\n",
    "        return self._next_observation()\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return np.array([self.data['Close'].iloc[self.current_step] / self.data['Close'].max()])\n",
    "\n",
    "    def step(self, action):\n",
    "        assert self.action_space.contains(action)\n",
    "\n",
    "        self.current_step += 1\n",
    "\n",
    "        if action == 1:  # Comprar\n",
    "            if self.holdings == 0:\n",
    "                self.holdings = self.balance / self.data['Close'].iloc[self.current_step]\n",
    "                self.balance = 0\n",
    "\n",
    "        elif action == 2:  # Vender\n",
    "            if self.holdings > 0:\n",
    "                self.balance = self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "                self.holdings = 0\n",
    "\n",
    "        # Calcular a recompensa\n",
    "        current_balance = self.balance + self.holdings * self.data['Close'].iloc[self.current_step]\n",
    "        self.profit = (current_balance - self.initial_balance) / self.initial_balance\n",
    "\n",
    "        # Verificar se o episódio terminou (pode adicionar outras condições de término)\n",
    "        done = self.current_step == len(self.data) - 1\n",
    "\n",
    "        # Definir a recompensa\n",
    "        if done:\n",
    "            reward = self.profit\n",
    "        else:\n",
    "            reward = self.profit - 0.001  # Penalidade diária para incentivar ações mais curtas\n",
    "\n",
    "        return self._next_observation(), reward, done, {}\n",
    "\n",
    "# Carregar os dados históricos do arquivo CSV\n",
    "data = pd.read_csv('data_/AAPL.csv')\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Criação do ambiente de trading\n",
    "env = TradingEnv(data)\n",
    "\n",
    "# Implementação do algoritmo Deep Q-Network (DQN)\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95  # Fator de desconto\n",
    "        self.epsilon = 1.0  # Probabilidade de exploração inicial\n",
    "        self.epsilon_decay = 0.995  # Taxa de decaimento da probabilidade de exploração\n",
    "        self.epsilon_min = 0.01  # Probabilidade mínima de exploração\n",
    "        self.learning_rate = 0.001\n",
    "        self.model = self._build_model()\n",
    "\n",
    "    def _build_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_dim=self.state_size, activation='relu'))\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(self.action_size, activation='linear'))\n",
    "        model.compile(loss='mse', optimizer=Adam(lr=self.learning_rate))\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return np.random.choice(self.action_size)\n",
    "        act_values = self.model.predict(state)\n",
    "        return np.argmax(act_values[0])\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])\n",
    "            target_f = self.model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Configuração do agente DQN\n",
    "state_size = 1\n",
    "action_size = 3\n",
    "batch_size = 32\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "# Treinamento do agente DQN\n",
    "for e in range(100):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, state_size])\n",
    "    for time in range(500):\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        next_state = np.reshape(next_state, [1, state_size])\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        if done:\n",
    "            print(\"Época: {}/{}, Lucro: {:.2f}\"\n",
    "                  .format(e, 100, env.profit))\n",
    "            break\n",
    "    if len(agent.memory) > batch_size:\n",
    "        agent.replay(batch_size)\n",
    "\n",
    "# Teste da estratégia aprendida\n",
    "state = env.reset()\n",
    "state = np.reshape(state, [1, state_size])\n",
    "done = False\n",
    "while not done:\n",
    "    action = agent.act(state)\n",
    "    next_state, _, done, _ = env.step(action)\n",
    "    state = np.reshape(next_state, [1, state_size])\n",
    "\n",
    "final_profit = env.profit\n",
    "print(f\"Lucro final da estratégia: {final_profit:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
